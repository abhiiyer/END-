{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test_Acc_3672.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNSl+sz2XgsjMJInV3P7aHR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhiiyer/END-/blob/main/Assignment7/Test_Acc_3672.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcgiUI3-BiM8"
      },
      "source": [
        "import copy\r\n",
        "import torch #deal with tensors\r\n",
        "from torch import nn\r\n",
        "from torch import optim\r\n",
        "import torchtext\r\n",
        "from torchtext import data #handling text data\r\n",
        "from torchtext import datasets"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMBqJgDtBiQB",
        "outputId": "a4e34998-94d7-45ed-d98d-904388e2cdd1"
      },
      "source": [
        "#Reproducing same results\r\n",
        "SEED = 2019\r\n",
        "\r\n",
        "#Torch\r\n",
        "torch.manual_seed(SEED)\r\n",
        "\r\n",
        "#Cuda algorithms\r\n",
        "torch.backends.cudnn.deterministic = True  \r\n",
        "\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "\r\n",
        "TEXT = data.Field(sequential=True, batch_first=True, lower=True, include_lengths = True)\r\n",
        "LABEL = data.LabelField()\r\n",
        "\r\n",
        "# load data splits\r\n",
        "train_data, val_data, test_data = datasets.SST.splits(TEXT, LABEL, fine_grained=True)\r\n",
        "print((len(train_data), len(val_data), len(test_data)))\r\n",
        "print(vars(train_data.examples[0]))\r\n",
        "\r\n",
        "\r\n",
        "#initialize glove embeddings\r\n",
        "\r\n",
        "TEXT.build_vocab(train_data,  max_size = 25_000, \r\n",
        "                 vectors = \"glove.6B.300d\", \r\n",
        "                 unk_init = torch.Tensor.normal_)\r\n",
        "\r\n",
        "LABEL.build_vocab(train_data)\r\n",
        "\r\n",
        "#No. of unique tokens in text\r\n",
        "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\r\n",
        "\r\n",
        "#No. of unique tokens in label\r\n",
        "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\r\n",
        "\r\n",
        "#Commonly used words\r\n",
        "print(TEXT.vocab.freqs.most_common(10))  \r\n",
        "\r\n",
        "#Word dictionary\r\n",
        "#print(TEXT.vocab.stoi)   \r\n",
        "\r\n",
        "#check whether cuda is available\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \r\n",
        "\r\n",
        "#set batch size\r\n",
        "BATCH_SIZE = 64\r\n",
        "\r\n",
        "#Load an iterator\r\n",
        "train_iterator, valid_iterator, test_iterator  = data.BucketIterator.splits(\r\n",
        "    (train_data, val_data, test_data), \r\n",
        "    batch_size = BATCH_SIZE,\r\n",
        "    sort_key = lambda x: len(x.text),\r\n",
        "    sort_within_batch=True,\r\n",
        "    device = device)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8544, 1101, 2210)\n",
            "{'text': ['the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'century', \"'s\", 'new', '``', 'conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarzenegger', ',', 'jean-claud', 'van', 'damme', 'or', 'steven', 'segal', '.'], 'label': 'positive'}\n",
            "Size of TEXT vocabulary: 16581\n",
            "Size of LABEL vocabulary: 5\n",
            "[('.', 8024), ('the', 7303), (',', 7131), ('a', 5281), ('and', 4473), ('of', 4396), ('to', 3021), ('is', 2561), (\"'s\", 2544), ('it', 2422)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-BlJjmNCQwP"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "\r\n",
        "class classifier(nn.Module):\r\n",
        "    \r\n",
        "    #define all the layers used in model\r\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \r\n",
        "                 bidirectional, dropout):\r\n",
        "        \r\n",
        "        #Constructor\r\n",
        "        super().__init__()          \r\n",
        "        \r\n",
        "        #embedding layer\r\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\r\n",
        "        \r\n",
        "        #lstm layer\r\n",
        "        self.lstm = nn.LSTM(embedding_dim, \r\n",
        "                           hidden_dim, \r\n",
        "                           num_layers=n_layers, \r\n",
        "                           bidirectional=bidirectional, \r\n",
        "                           dropout=dropout,\r\n",
        "                           batch_first=True)\r\n",
        "        \r\n",
        "        #dense layer\r\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\r\n",
        "        \r\n",
        "        #activation function\r\n",
        "        self.act = nn.Softmax()\r\n",
        "        \r\n",
        "    def forward(self, text, text_lengths):\r\n",
        "        \r\n",
        "        #text = [batch size,sent_length]\r\n",
        "        embedded = self.embedding(text)\r\n",
        "        #embedded = [batch size, sent_len, emb dim]\r\n",
        "      \r\n",
        "        #packed sequence\r\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(),batch_first=True)\r\n",
        "        \r\n",
        "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\r\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\r\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\r\n",
        "        \r\n",
        "        #concat the final forward and backward hidden state\r\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\r\n",
        "                \r\n",
        "        #hidden = [batch size, hid dim * num directions]\r\n",
        "        dense_outputs=self.fc(hidden)\r\n",
        "\r\n",
        "        #Final activation function\r\n",
        "        outputs=self.act(dense_outputs)\r\n",
        "        \r\n",
        "        return outputs\r\n",
        "\r\n",
        "#define hyperparameters\r\n",
        "size_of_vocab = len(TEXT.vocab)\r\n",
        "embedding_dim = 300\r\n",
        "num_hidden_nodes = 32\r\n",
        "num_output_nodes = len(LABEL.vocab)\r\n",
        "num_layers = 2\r\n",
        "bidirection = True\r\n",
        "dropout = 0.2\r\n",
        "\r\n",
        "#instantiate the model\r\n",
        "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, num_layers, \r\n",
        "                   bidirectional = True, dropout = dropout)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMP600yYCzFR",
        "outputId": "0c533d6f-574c-470d-9dab-78b9050886b2"
      },
      "source": [
        "#architecture\r\n",
        "print(model)\r\n",
        "\r\n",
        "#No. of trianable parameters\r\n",
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "    \r\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\r\n",
        "\r\n",
        "#Initialize the pretrained embedding\r\n",
        "pretrained_embeddings = TEXT.vocab.vectors\r\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)\r\n",
        "\r\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\r\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\r\n",
        "\r\n",
        "\r\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(embedding_dim)\r\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(embedding_dim)\r\n",
        "\r\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifier(\n",
            "  (embedding): Embedding(16581, 300)\n",
            "  (lstm): LSTM(300, 32, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
            "  (fc): Linear(in_features=64, out_features=5, bias=True)\n",
            "  (act): Softmax(dim=None)\n",
            ")\n",
            "The model has 5,085,217 trainable parameters\n",
            "torch.Size([16581, 300])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_h_wEsmMC9NH"
      },
      "source": [
        "import torch.optim as optim\r\n",
        "\r\n",
        "#define optimizer and loss\r\n",
        "optimizer = optim.Adam(model.parameters())\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "#define metric\r\n",
        "def categorical_accuracy(preds, y):\r\n",
        "    \"\"\"\r\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\r\n",
        "    \"\"\"\r\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\r\n",
        "    correct = max_preds.squeeze(1).eq(y)\r\n",
        "    return correct.sum().to(device=\"cpu\") / torch.FloatTensor([y.shape[0]])\r\n",
        "    \r\n",
        "#push to cuda if available\r\n",
        "model = model.to(device)\r\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGjmo4BADTGr"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\r\n",
        "    \r\n",
        "    #initialize every epoch \r\n",
        "    epoch_loss = 0\r\n",
        "    epoch_acc = 0\r\n",
        "    \r\n",
        "    #set the model in training phase\r\n",
        "    model.train()  \r\n",
        "    \r\n",
        "    for batch in iterator:\r\n",
        "        \r\n",
        "        #resets the gradients after every batch\r\n",
        "        optimizer.zero_grad()   \r\n",
        "        \r\n",
        "        #retrieve text and no. of words\r\n",
        "        text, text_lengths = batch.text   \r\n",
        "        \r\n",
        "        #convert to 1D tensor\r\n",
        "        predictions = model(text, text_lengths).squeeze()  \r\n",
        "        \r\n",
        "        #compute the loss\r\n",
        "        loss = criterion(predictions, batch.label)        \r\n",
        "        \r\n",
        "        #compute the binary accuracy\r\n",
        "        acc = categorical_accuracy(predictions, batch.label)   \r\n",
        "        \r\n",
        "        #backpropage the loss and compute the gradients\r\n",
        "        loss.backward()       \r\n",
        "        \r\n",
        "        #update the weights\r\n",
        "        optimizer.step()      \r\n",
        "        \r\n",
        "        #loss and accuracy\r\n",
        "        epoch_loss += loss.item()  \r\n",
        "        epoch_acc += acc.item()    \r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCpRL96nDT1F"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\r\n",
        "    \r\n",
        "    #initialize every epoch\r\n",
        "    epoch_loss = 0\r\n",
        "    epoch_acc = 0\r\n",
        "\r\n",
        "    #deactivating dropout layers\r\n",
        "    model.eval()\r\n",
        "    \r\n",
        "    #deactivates autograd\r\n",
        "    with torch.no_grad():\r\n",
        "    \r\n",
        "        for batch in iterator:\r\n",
        "        \r\n",
        "            #retrieve text and no. of words\r\n",
        "            text, text_lengths = batch.text\r\n",
        "            \r\n",
        "            #convert to 1d tensor\r\n",
        "            predictions = model(text, text_lengths).squeeze()\r\n",
        "            \r\n",
        "            #compute loss and accuracy\r\n",
        "            loss = criterion(predictions, batch.label)\r\n",
        "            acc = categorical_accuracy(predictions, batch.label)\r\n",
        "            \r\n",
        "            #keep track of loss and accuracy\r\n",
        "            epoch_loss += loss.item()\r\n",
        "            epoch_acc += acc.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcHT3uQNDYh_",
        "outputId": "f8711aa4-f1a0-4a59-bd19-f2a26de72e3e"
      },
      "source": [
        "N_EPOCHS = 25\r\n",
        "best_valid_loss = float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "     \r\n",
        "    #train the model\r\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\r\n",
        "    \r\n",
        "    #evaluate the model\r\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\r\n",
        "    \r\n",
        "    #save the best model\r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\r\n",
        "    \r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:49: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 1.562 | Train Acc: 31.23%\n",
            "\t Val. Loss: 1.521 |  Val. Acc: 37.55%\n",
            "\tTrain Loss: 1.476 | Train Acc: 42.01%\n",
            "\t Val. Loss: 1.499 |  Val. Acc: 38.69%\n",
            "\tTrain Loss: 1.414 | Train Acc: 47.81%\n",
            "\t Val. Loss: 1.491 |  Val. Acc: 38.50%\n",
            "\tTrain Loss: 1.355 | Train Acc: 55.11%\n",
            "\t Val. Loss: 1.513 |  Val. Acc: 37.47%\n",
            "\tTrain Loss: 1.304 | Train Acc: 60.32%\n",
            "\t Val. Loss: 1.516 |  Val. Acc: 37.73%\n",
            "\tTrain Loss: 1.265 | Train Acc: 64.32%\n",
            "\t Val. Loss: 1.540 |  Val. Acc: 34.86%\n",
            "\tTrain Loss: 1.227 | Train Acc: 67.98%\n",
            "\t Val. Loss: 1.523 |  Val. Acc: 37.63%\n",
            "\tTrain Loss: 1.201 | Train Acc: 70.49%\n",
            "\t Val. Loss: 1.522 |  Val. Acc: 37.38%\n",
            "\tTrain Loss: 1.184 | Train Acc: 72.18%\n",
            "\t Val. Loss: 1.529 |  Val. Acc: 36.34%\n",
            "\tTrain Loss: 1.167 | Train Acc: 73.82%\n",
            "\t Val. Loss: 1.531 |  Val. Acc: 35.83%\n",
            "\tTrain Loss: 1.149 | Train Acc: 75.85%\n",
            "\t Val. Loss: 1.538 |  Val. Acc: 36.08%\n",
            "\tTrain Loss: 1.130 | Train Acc: 77.69%\n",
            "\t Val. Loss: 1.535 |  Val. Acc: 36.00%\n",
            "\tTrain Loss: 1.121 | Train Acc: 78.45%\n",
            "\t Val. Loss: 1.517 |  Val. Acc: 38.15%\n",
            "\tTrain Loss: 1.109 | Train Acc: 79.61%\n",
            "\t Val. Loss: 1.535 |  Val. Acc: 36.08%\n",
            "\tTrain Loss: 1.101 | Train Acc: 80.53%\n",
            "\t Val. Loss: 1.550 |  Val. Acc: 34.19%\n",
            "\tTrain Loss: 1.095 | Train Acc: 81.03%\n",
            "\t Val. Loss: 1.530 |  Val. Acc: 36.87%\n",
            "\tTrain Loss: 1.078 | Train Acc: 82.71%\n",
            "\t Val. Loss: 1.521 |  Val. Acc: 37.56%\n",
            "\tTrain Loss: 1.071 | Train Acc: 83.31%\n",
            "\t Val. Loss: 1.536 |  Val. Acc: 36.35%\n",
            "\tTrain Loss: 1.066 | Train Acc: 84.01%\n",
            "\t Val. Loss: 1.535 |  Val. Acc: 36.00%\n",
            "\tTrain Loss: 1.064 | Train Acc: 84.04%\n",
            "\t Val. Loss: 1.542 |  Val. Acc: 35.57%\n",
            "\tTrain Loss: 1.055 | Train Acc: 84.89%\n",
            "\t Val. Loss: 1.521 |  Val. Acc: 37.48%\n",
            "\tTrain Loss: 1.051 | Train Acc: 85.40%\n",
            "\t Val. Loss: 1.532 |  Val. Acc: 36.26%\n",
            "\tTrain Loss: 1.048 | Train Acc: 85.75%\n",
            "\t Val. Loss: 1.532 |  Val. Acc: 36.26%\n",
            "\tTrain Loss: 1.044 | Train Acc: 86.08%\n",
            "\t Val. Loss: 1.522 |  Val. Acc: 37.39%\n",
            "\tTrain Loss: 1.040 | Train Acc: 86.46%\n",
            "\t Val. Loss: 1.525 |  Val. Acc: 37.31%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0iR1NskG26e",
        "outputId": "adc99fe2-f4cc-4d2e-f8be-78f8539999bf"
      },
      "source": [
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\r\n",
        "\r\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 1.528 | Test Acc: 36.77%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:49: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcKMztcGHtzi"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    }
  ]
}